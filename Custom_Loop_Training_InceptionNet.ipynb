{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Inception Modeling  with Model Subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 2070 SUPER, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# enable mixed precision\n",
    "mixed_precision = True\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "# Conv Module \n",
    "class ConvModule(tf.keras.layers.Layer):\n",
    "\tdef __init__(self, kernel_num, \n",
    "                 kernel_size, strides, padding='same'):\n",
    "\t\tsuper(ConvModule, self).__init__()\n",
    "\t\tself.conv = tf.keras.layers.Conv2D(kernel_num, kernel_size=kernel_size, strides=strides, padding=padding)\n",
    "\t\tself.bn   = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "\tdef call(self, input_tensor, training=False):\n",
    "\t\tx = self.conv(input_tensor)\n",
    "\t\tx = self.bn(x, training=training)\n",
    "\t\tx = tf.nn.relu(x)\n",
    "\t\treturn x \n",
    "\n",
    "# Inception Module\n",
    "class InceptionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size1x1, kernel_size3x3):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.conv1 = ConvModule(kernel_size1x1, kernel_size=(1,1), strides=(1,1))\n",
    "        self.conv2 = ConvModule(kernel_size3x3, kernel_size=(3,3), strides=(1,1))\n",
    "        self.cat   = tf.keras.layers.Concatenate()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x_1x1 = self.conv1(input_tensor)\n",
    "        x_3x3 = self.conv2(input_tensor)\n",
    "        x = self.cat([x_1x1, x_3x3])\n",
    "        return x \n",
    "\n",
    "# Downsample Module \n",
    "class DownsampleModule(tf.keras.layers.Layer):\n",
    "\tdef __init__(self, kernel_size):\n",
    "\t\tsuper(DownsampleModule, self).__init__()\n",
    "\t\tself.conv3 = ConvModule(kernel_size, kernel_size=(3,3), strides=(2,2), padding=\"valid\")\n",
    "\t\tself.pool  = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2,2))\n",
    "\t\tself.cat   = tf.keras.layers.Concatenate()\n",
    "\n",
    "\tdef call(self, input_tensor, training=False):\n",
    "\t\tconv_x = self.conv3(input_tensor, training=training)\n",
    "\t\tpool_x = self.pool(input_tensor)\n",
    "\t\treturn self.cat([conv_x, pool_x])\n",
    "\n",
    "# Encompassing Module in Model Class\n",
    "class MiniInception(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MiniInception, self).__init__()\n",
    "        self.conv_block = ConvModule(96, (3,3), (1,1))\n",
    "        self.inception_block1 = InceptionModule(32, 32)\n",
    "        self.inception_block2 = InceptionModule(32, 48)\n",
    "        self.downsample_block1 = DownsampleModule(80)\n",
    "  \n",
    "        self.inception_block3 = InceptionModule(112, 48)\n",
    "        self.inception_block4 = InceptionModule(96, 64)\n",
    "        self.inception_block5 = InceptionModule(80, 80)\n",
    "        self.inception_block6 = InceptionModule(48, 96)\n",
    "        self.downsample_block2 = DownsampleModule(96)\n",
    "\n",
    "        self.inception_block7 = InceptionModule(176, 160)\n",
    "        self.inception_block8 = InceptionModule(176, 160)\n",
    "\n",
    "        self.avg_pool = tf.keras.layers.AveragePooling2D((7,7))\n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.classfier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "\n",
    "    def call(self, input_tensor, training=False,**kwargs):\n",
    "        x = self.conv_block(input_tensor)\n",
    "        x = self.inception_block1(x)\n",
    "        x = self.inception_block2(x)\n",
    "        x = self.downsample_block1(x)\n",
    "\n",
    "        x = self.inception_block3(x)\n",
    "        x = self.inception_block4(x)\n",
    "        x = self.inception_block5(x)\n",
    "        x = self.inception_block6(x)\n",
    "        x = self.downsample_block2(x)\n",
    "\n",
    "        x = self.inception_block7(x)\n",
    "        x = self.inception_block8(x)\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "        return self.classfier(x)\n",
    "\n",
    "    def build_graph(self, shape):\n",
    "        x = tf.keras.layers.Input(shape=shape)\n",
    "        return Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = (32, 32, 3)\n",
    "\n",
    "cm = MiniInception()\n",
    "y  = cm(tf.ones(shape=(0,*raw_input))) # The first call to the `cm` will create the weights\n",
    "\n",
    "cm.build_graph(raw_input).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set (CIFAR-10) and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1)\n",
      "(10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# train set / data \n",
    "x_train = x_train.astype('float32') / 255\n",
    "# validation set / data \n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[y_train[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(train_dataset):\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "\n",
    "for i, (x, y) in enumerate(val_dataset):\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loop Training from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 0.78 - epoch: 1 loss: 0.7587890625  acc: 0.5794399976730347 val loss: 3.173828125 val acc: 0.10159999877214432\n",
      "\n",
      "ETA: 0.29 - epoch: 2 loss: 0.63232421875  acc: 0.7421200275421143 val loss: 1.0126953125 val acc: 0.5756999850273132\n",
      "\n",
      "ETA: 0.32 - epoch: 3 loss: 0.453369140625  acc: 0.8073400259017944 val loss: 0.7734375 val acc: 0.7243000268936157\n",
      "\n",
      "ETA: 0.33 - epoch: 4 loss: 0.474365234375  acc: 0.8501200079917908 val loss: 0.64111328125 val acc: 0.7628999948501587\n",
      "\n",
      "ETA: 0.4 - epoch: 5 loss: 0.28369140625  acc: 0.8811399936676025 val loss: 0.65673828125 val acc: 0.6589000225067139\n",
      "\n",
      "ETA: 0.34 - epoch: 6 loss: 0.288330078125  acc: 0.906059980392456 val loss: 0.794921875 val acc: 0.7311000227928162\n",
      "\n",
      "ETA: 0.53 - epoch: 7 loss: 0.1297607421875  acc: 0.9277999997138977 val loss: 0.91162109375 val acc: 0.7084000110626221\n",
      "\n",
      "ETA: 0.31 - epoch: 8 loss: 0.0743408203125  acc: 0.9464399814605713 val loss: 0.5654296875 val acc: 0.7071999907493591\n",
      "\n",
      "ETA: 0.3 - epoch: 9 loss: 0.0855712890625  acc: 0.9574599862098694 val loss: 1.068359375 val acc: 0.7073000073432922\n",
      "\n",
      "ETA: 0.31 - epoch: 10 loss: 0.07928466796875  acc: 0.9668400287628174 val loss: 0.58251953125 val acc: 0.7400000095367432\n",
      "\n",
      "ETA: 0.31 - epoch: 11 loss: 0.09954833984375  acc: 0.9705399870872498 val loss: 1.2607421875 val acc: 0.6661999821662903\n",
      "\n",
      "ETA: 0.34 - epoch: 12 loss: 0.07501220703125  acc: 0.9763799905776978 val loss: 0.95458984375 val acc: 0.7415000200271606\n",
      "\n",
      "ETA: 0.51 - epoch: 13 loss: 0.05450439453125  acc: 0.9802600145339966 val loss: 0.87646484375 val acc: 0.7635999917984009\n",
      "\n",
      "ETA: 0.54 - epoch: 14 loss: 0.033935546875  acc: 0.9846799969673157 val loss: 1.3583984375 val acc: 0.7135999798774719\n",
      "\n",
      "ETA: 0.48 - epoch: 15 loss: 0.01294708251953125  acc: 0.9850800037384033 val loss: 1.30859375 val acc: 0.7185999751091003\n",
      "\n",
      "ETA: 0.35 - epoch: 16 loss: 0.07281494140625  acc: 0.9881600141525269 val loss: 1.2041015625 val acc: 0.7530999779701233\n",
      "\n",
      "ETA: 0.35 - epoch: 17 loss: 0.0443115234375  acc: 0.9857199788093567 val loss: 1.8603515625 val acc: 0.7465000152587891\n",
      "\n",
      "ETA: 0.68 - epoch: 18 loss: 0.01328277587890625  acc: 0.9839400053024292 val loss: 0.65380859375 val acc: 0.7875999808311462\n",
      "\n",
      "ETA: 0.53 - epoch: 19 loss: 0.035552978515625  acc: 0.9851599931716919 val loss: 1.0849609375 val acc: 0.7432000041007996\n",
      "\n",
      "ETA: 0.4 - epoch: 20 loss: 0.04217529296875  acc: 0.9877399802207947 val loss: 3.078125 val acc: 0.7224000096321106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# train set / target \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "# validation set / target \n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# define model \n",
    "model = MiniInception()\n",
    "model.build_graph().summary()\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric   = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# tensorboard writer \n",
    "train_writer = tf.summary.create_file_writer('logs/train/')\n",
    "test_writer  = tf.summary.create_file_writer('logs/test/')\n",
    "\n",
    "@tf.function\n",
    "def train_step(step, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        train_loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(train_loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    \n",
    "    # write training loss and accuracy to the tensorboard\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss_value, step=step)\n",
    "        tf.summary.scalar('accuracy', train_acc_metric.result(), step=step) \n",
    "    \n",
    "    return train_loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(step, x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    # Compute the loss value f\n",
    "    val_loss_value = loss_fn(y, val_logits)\n",
    "    # Update val metrics\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "    \n",
    "    # write test loss and accuracy to the tensorboard\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('val loss', val_loss_value, step=step)\n",
    "        tf.summary.scalar('val accuracy', val_acc_metric.result(), step=step) \n",
    "\n",
    "    return val_loss_value\n",
    "\n",
    "# custom training loop \n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    # batch training \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for train_batch_step, (x_batch_train,\\\n",
    "                           y_batch_train) in enumerate(train_dataset):\n",
    "        train_batch_step = tf.convert_to_tensor(train_batch_step, dtype=tf.int64)\n",
    "        train_loss_value = train_step(train_batch_step, x_batch_train, y_batch_train)\n",
    "\n",
    "    # evaluation on validation set \n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for test_batch_step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "        test_batch_step = tf.convert_to_tensor(test_batch_step, dtype=tf.int64)\n",
    "        val_loss_value = test_step(test_batch_step, x_batch_val, y_batch_val)\n",
    "\n",
    "    template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "    print(template.format(\n",
    "        round((time.time() - t)/60, 2), epoch + 1,\n",
    "        train_loss_value, float(train_acc_metric.result()),\n",
    "        val_loss_value, float(val_acc_metric.result())\n",
    "    ))\n",
    "        \n",
    "    # Reset metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "    val_acc_metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
